# -*- coding: utf-8 -*-
"""SEC - 101_Autos_Radit Srisathaporn (Vic).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CPk4J86Sd-ZLESGuEvYwRD26O9nV3OVx
"""

#Cell1: Install required libraries
!pip -q install scikit-learn pandas matplotlib

#Cell1: Imports
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import numpy as np
import random
np.random.seed(42)
random.seed(42)

from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, r2_score

#Cell2: Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

#Cell2: Path configuration
#IMPORTANT: Change this path to where autos.csv is stored in YOUR Drive
DATA_PATH = "/content/drive/MyDrive/autos.csv"

#Cell2: Load dataset
#Note: autos.csv uses latin-1 encoding (common for this dataset)
autos_raw = pd.read_csv(DATA_PATH, encoding="latin1")

print("autos.csv loaded from Google Drive")
print("Shape (rows, columns):", autos_raw.shape)
print("\nColumns:")
print(autos_raw.columns.tolist())

#Cell3: Basic dataset info & 10 sample records (RAW)
print("=== DataFrame .info() ===")
autos_raw.info()

print("\n=== First 10 records (RAW dataset) ===")
sample_raw = autos_raw.head(10).copy()
display(sample_raw)

#Keep a copy of these exact 10 rows by index for later comparison
sample_raw_indices = sample_raw.index.to_list()
print("\nStored indices of the 10 sample records:", sample_raw_indices)

#Cell4: Dataset analysis (RAW)

#Number of records and attributes
num_rows, num_cols = autos_raw.shape
print(f"Number of records (rows): {num_rows}")
print(f"Number of attributes (columns): {num_cols}")

#Missing values per column
print("\n=== Missing values per column (RAW) ===")
missing_raw = autos_raw.isna().sum().sort_values(ascending=False)
display(missing_raw)

#Basic stats for numeric columns
print("\n=== Numeric columns descriptive statistics (RAW) ===")
display(autos_raw.describe())

#Basic overview of categorical columns (some examples)
categorical_cols = autos_raw.select_dtypes(include=["object"]).columns.tolist()
print("\nNumber of categorical columns:", len(categorical_cols))
print("Example categorical columns:", categorical_cols[:10])

#Unique counts for some key categorical attributes
for col in ["brand", "vehicleType", "gearbox", "fuelType", "notRepairedDamage"]:
    if col in autos_raw.columns:
        print(f"\n=== Value counts for {col} (top 10) ===")
        display(autos_raw[col].value_counts(dropna=False).head(10))

#Cell5: Focused quality checks for key numeric fields

important_numeric = ["yearOfRegistration", "price", "powerPS", "kilometer"]
print("=== Descriptive stats for key numeric fields (RAW) ===")
display(autos_raw[important_numeric].describe())

#Check min/max values (useful to justify cleaning thresholds)
print("\n=== Min/Max of key numeric fields ===")
for col in important_numeric:
    print(f"{col}: min={autos_raw[col].min()}, max={autos_raw[col].max()}")

#Check for duplicate rows
num_duplicates = autos_raw.duplicated().sum()
print(f"\nNumber of fully duplicated rows: {num_duplicates}")

#Cell6: Data cleaning & preprocessing

autos = autos_raw.copy()

print("Initial number of rows:", len(autos))

#Step1: Remove fully duplicated rows
before = len(autos)
autos = autos.drop_duplicates()
after = len(autos)
print(f"Step1 - Drop duplicates: {before} -> {after} rows")

#Step2: Filter unrealistic registration years
#We keep cars registered between 1950 and 2016 (dataset is from 2016)
before = len(autos)
autos = autos[(autos["yearOfRegistration"] >= 1950) & (autos["yearOfRegistration"] <= 2016)]
after = len(autos)
print(f"Step2 - Filter yearOfRegistration (1950-2016): {before} -> {after} rows")

#Step3: Filter unrealistic prices
#Keep cars with price between 100 and 100000 (Euro). This removes zeros and extreme outliers.
before = len(autos)
autos = autos[(autos["price"] >= 100) & (autos["price"] <= 100000)]
after = len(autos)
print(f"Step3 - Filter price (100 to 100000): {before} -> {after} rows")

#Step4: Filter unrealistic powerPS values
#Keep cars with power between 10 PS and 500 PS (removes 0 and extreme errors)
before = len(autos)
autos = autos[(autos["powerPS"] >= 10) & (autos["powerPS"] <= 500)]
after = len(autos)
print(f"Step4 - Filter powerPS (10 to 500): {before} -> {after} rows")

#Step5: Handle missing values for modeling
#For the model, we will use the following features:
model_features = [
    "yearOfRegistration",
    "kilometer",
    "powerPS",
    "gearbox",
    "vehicleType",
    "brand",
    "fuelType",
    "notRepairedDamage"
]

#Subset to features + target
autos_model = autos[model_features + ["price"]].copy()

#Drop rows with any missing values in these columns
before = len(autos_model)
autos_model = autos_model.dropna(subset=model_features + ["price"])
after = len(autos_model)
print(f"Step5 - Drop rows with missing values in model features: {before} -> {after} rows")

print("\nFinal shape of cleaned dataset used for modeling:", autos_model.shape)

#Cell7: Show the 10 sample records AFTER CLEANING

print("Original indices of the 10 sample records (RAW):", sample_raw_indices)

#We check which of the original 10 records survived in autos_model
existing_indices = [idx for idx in sample_raw_indices if idx in autos_model.index]
print("Indices that survived in the final modeling dataset:", existing_indices)

#Retrieve the cleaned records that still exist
sample_cleaned = autos_model.loc[existing_indices].copy()

#If fewer than 10 survived, add more clean rows
if len(sample_cleaned) < 10:
    needed = 10 - len(sample_cleaned)

    #Get additional valid rows (exclude existing)
    additional_rows = autos_model.drop(index=existing_indices).head(needed)

    #Combine the survivors + new clean rows
    sample_cleaned = pd.concat([sample_cleaned, additional_rows], ignore_index=False)

print("\n=== Final 10 sample records AFTER CLEANING (from autos_model) ===")
display(sample_cleaned)

print("\nNumber of sample records after adjustment:", len(sample_cleaned))

#Cell8: Fast Random Forest Regressor

from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.metrics import mean_absolute_error, r2_score
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestRegressor

#Separate features (X) and target (y)
X = autos_model[model_features]
y = autos_model["price"]

#Numeric + Categorical columns
numeric_features = ["yearOfRegistration", "kilometer", "powerPS"]
categorical_features = ["gearbox", "vehicleType", "brand", "fuelType", "notRepairedDamage"]

print("Numeric:", numeric_features)
print("Categorical:", categorical_features)

#Preprocessing
preprocessor = ColumnTransformer(
    transformers=[
        ("num", "passthrough", numeric_features),
        ("cat", OneHotEncoder(handle_unknown="ignore"), categorical_features),
    ]
)

#Random Forest
model = RandomForestRegressor(
    n_estimators=30,
    max_depth=20,
    random_state=42,
    n_jobs=-1
)

#Split
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=42
)

print("Train size:", X_train.shape, "Test size:", X_test.shape)

#Pipeline
rf_pipeline = Pipeline(steps=[
    ("preprocessor", preprocessor),
    ("model", model)
])

#Train
rf_pipeline.fit(X_train, y_train)

#Predict
y_pred = rf_pipeline.predict(X_test)

#Evaluate
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("\n Model Performance (Random Forest)")
print(f"Mean Absolute Error (MAE): {mae:.2f}")
print(f"RÂ² Score: {r2:.3f}")

#Cell9: Save cleaned dataset used for modeling

CLEANED_PATH = "/content/drive/MyDrive/autos_cleaned_for_model.csv"

autos_model.to_csv(CLEANED_PATH, index=False)
print(f"Cleaned dataset saved to: {CLEANED_PATH}")